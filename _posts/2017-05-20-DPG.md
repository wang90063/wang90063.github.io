---
layout: post
title: Deterministic Policy Gradient
date: 2017-5-20
excerpt: "A detailed introduction to DPG"
comments: true
tags: [reinforcement learning,policy gradient] 
---

# Motivation

Deep Reinforcement Learning (DRL) is still very hot! The robots trained by DRL can beat professional human players in games, like [Atari](https://deepmind.com/research/dqn/), [Torcs](https://arxiv.org/abs/1509.02971), [Zoom](https://openreview.net/pdf?id=Hk3mPK5gg) and [Go](https://deepmind.com/research/alphago/). Also, DRL endows machines with the ability to [autonomously acquire the skills for executing complex tasks](https://people.eecs.berkeley.edu/~svlevine/). In the last year, I have been researching to combine the DRL controller with wireless communication network management, which can implement network planning, configuration, management, optimizaiton and healing simpler and faster. My tutorials to learn DRL includes [Richard Sutton's book](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf), [David Silver's cources](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), [Andrej Karpathy's blog](http://karpathy.github.io/2016/05/31/rl/), [cs231n from Stanford](http://cs231n.stanford.edu/), [cs294 from UC Berkeley](http://rll.berkeley.edu/deeprlcourse/) and the [tutorial from NIPS 2016](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Reinforcement-Learning-Through-Policy-Optimization) .

As for the RL algorithms, I focus on policy gradient (PG) rather than value-based algorithm, e.g. Q-learning. It is because that PG can get [better performance](https://arxiv.org/pdf/1602.01783.pdf) than Q-learning when deep neural network is used as function approximators. Furthermore, [PG can handle the continuous action space](https://arxiv.org/abs/1509.02971). 

In this blog, I will introduce the reasons to propose deterministic policy gradient (DPG), how it works and is combinated with deep learning.

# Policy Gradient

First of all, I give a high-level definition for PG. PG is a model-free RL, which adjusts the policy parameters in the direction of greater expected reward. PG has two categories: Stochastic Policy Gradient (SPG) and Deterministic Policy Gradient (DPG).
  
## Stochastic Policy Gradient  

It is obvious that the policy in SPG is the probability distribution of action conditioned on the current state and also paramerized by $$ \theta $$, i.e. $$ { \pi _\theta }(s,a) = P( a \mid s;\theta )$$. To update the parameter $$ \theta $$, the [policy gradient theorem](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) is proposed to 
adjust the parameter $$ \theta $$ is in the direction of $${\nabla _\theta }J\left( \theta  \right)$$, which is given as 

$$\begin{equation}
{\nabla _\theta }J\left( \theta  \right) = {E_{s_0 \sim \rho ({s_0}), s \sim {d^\pi }(s),a \sim {\pi _\theta }(s,a)}}\left[ {Q^\pi (s,a){\nabla _\theta }\log {\pi _\theta }(s,a)} \right]
\end{equation}$$

Intuitively, the gradients $${\nabla _\theta \log {\pi _\theta }(s,a)}$$ give the direction in the parameter space leading to the increase of policy probability assigned to action. Also, $$Q^\pi (s,a)$$ is the score of the action. Next, some mathematical proof of the policy gradient is given

### Policy Gradient Theorem Proof

This proof is refered to [this paper](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf). Furthermore, another proof from the perspective of trajectory is presented in [cs294](http://rll.berkeley.edu/deeprlcourse/).

Generally, RL is to derive the optimal policy to maximize the accumulative reward.  From the view of optimization, RL is expressed as

$$
\begin{equation}
\mathop {\pi ^*} = \arg\max\limits_\pi  {E_\pi }\left[ {r_0^\gamma \mid {s_0}}  \right]
\end{equation}
$$

where $$r_0^\gamma  = \sum\limits_{t = 0}^T {\gamma ^t{r_t}}$$ is the fixed-horizon reward in a episode. Then, two value functions are give as 

$$ \begin{equation}
{Q^\pi }\left( {s,a} \right) = {E_\pi }\left[ {r_t^\gamma \mid {s_t} = s,{a_t} = a} \right]
\end{equation}$$ 

and 

$$ \begin{equation}
{V^{\pi} }\left( {s,a} \right) = {E_\pi }\left[ {r_t^\gamma \mid {s_t} = s} \right]
\end{equation}$$

where $$r_t^\gamma  = \sum\limits_{k = t}^{T - 1} {\gamma ^{t - k}{r_{k + 1}}}$$ is the accumulative reward from time step $$t$$. Also $${V^\pi }\left( s \right) = \sum\limits_a {\pi (s,a){Q^\pi }\left( {s,a} \right)} $$. Moreover, the policy $$\pi (s,a)$$ is parameterized by $$\theta$$, i.e. $$\pi_\theta (s,a)$$. Also, the policy averaged reward is defined as

$$
\begin{equation}
R(s,s',a) = {E_\pi }\left[ {r_t|{s_t} = s,{a_t} = a,{s_{t + 1}} = s'} \right]
\end{equation}
$$ 

OK!! Now, I will give the proof of the policy gradient theorem with above preliminaries. 

$$
\begin{array}{l}
{\nabla _\theta }{V^{\pi _\theta }}\left( s \right) = {\nabla _\theta }\left[ {\sum\limits_a {\pi _\theta (s,a){Q^{\pi _\theta }}\left( {s,a} \right)} } \right]\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \sum\limits_a {\nabla _\theta {\pi _\theta }(s,a){Q^{\pi _\theta }}\left( {s,a} \right)}  + \sum\limits_a {\pi _\theta (s,a){\nabla _\theta }{Q^{\pi _\theta }}\left( {s,a} \right)} \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \sum\limits_a {\nabla _\theta {\pi _\theta }(s,a){Q^{\pi _\theta }}\left( {s,a} \right)}  + \gamma \sum\limits_a {\pi _\theta (s,a)\sum\limits_{s'} {P(s'\mid s,a){\nabla _\theta }{V^{\pi _\theta }}\left( {s'} \right)} } \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \sum\limits_x {\sum\limits_{k = 0}^{T - 1} {\gamma ^k P(s \to x,k,\pi _\theta } )} \sum\limits_a {\nabla _\theta {\pi _\theta }(x,a){Q^{\pi _\theta }}\left( {x,a} \right)} \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \sum\limits_s {d^{\pi _\theta }(s)} \sum\limits_a {\nabla _\theta {\pi _\theta }(s,a){Q^{\pi _\theta }}\left( {s,a} \right)} 
\end{array}
$$
where $$x$$ is the state $$k$$ time steps after $$s$$ and $$P(s \to x,k,\pi _\theta)$$ is the $$k-step$$ transmit probability averaged over policy, which is decomposed with transmit probability and policy due to the Markov property of the states $$P(s \to x,k,\pi _\theta)=\prod\limits_{t = 0}^{k - 1} {P({s_{t + 1}} \mid {s_t},{a_t}){\pi _\theta }({s_t},{a_t})}$$.

Furthermore, as mentioned above, the objective is the fixed-horizon reward started from $$s_0$$. Hence, the policy gradient is along with the direction to maximize $${E_\pi }\left[ {r_0^\gamma \mid {s_0}}\right]$$, i.e.

$$
\begin{array}{l}
{\nabla _\theta }{E_{\pi _\theta}}\left[ {r_t^\gamma \mid {s_t} = {s_0}} \right] = {\nabla _\theta }{V^{\pi _\theta }}\left( {s_0} \right)\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = {E_{s_0 \sim \rho ({s_0}),s \sim {d^{\pi _\theta }}(s),a \sim {\pi _\theta }(s, \bullet )}}\left[ {\nabla _\theta {\pi _\theta }(s,a){Q^{\pi _\theta }}\left( {s,a} \right)} \right]
\end{array}
$$

Now, the policy gradient theorem is derived.

#### Another way to prove 

Acutually, policy Gradients are a special case of a more general score function gradient estimator, which is given as

$$
\begin{array}{l}
{\nabla _\theta }{E_{x \sim p(x;\theta )}}\left[ {f(x)} \right] = \int\limits_x {\nabla _\theta p(x;\theta )f(x)} dx\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \int\limits_x {p(x;\theta ){\nabla _\theta }\log p(x;\theta )f(x)} dx\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = {\nabla _\theta }{E_{x \sim p(x;\theta )}}\left[ {\nabla _\theta \log p(x;\theta )f(x)} \right]
\end{array}
$$

Of course, it can be written as discrete form as above. Each episode can be seen as a trajectory guided by policy. In other words, PG is to increase the probablity to obtain high fixed-horizon reward from single trajectory,i.e.

$$
\begin{equation}
{\nabla _\theta }{E_{\tau  \sim p(\tau ;\theta )}}\left[ R(\tau ) \right] = {\nabla _\theta }{E_{\tau  \sim p(\tau ;\theta )}}\left[ {\nabla _\theta \log p(\tau ;\theta )R(\tau )} \right]
\end{equation}
$$
 