---
layout: post
title: Deterministic Policy Gradient
date: 2017-5-20
excerpt: "A detailed introduction to DPG"
comments: true
---

# Motivation

Deep Reinforcement Learning (DRL) is still very hot! The robots trained by DRL can beat professional human players in games, like [Atari](https://deepmind.com/research/dqn/), [Torcs](https://arxiv.org/abs/1509.02971), [Zoom](https://openreview.net/pdf?id=Hk3mPK5gg) and [Go](https://deepmind.com/research/alphago/). Also, DRL endows machines with the ability to [autonomously acquire the skills for executing complex tasks](https://people.eecs.berkeley.edu/~svlevine/). In the last year, I have been researching to combine the DRL controller with wireless communication network management, which can implement network planning, configuration, management, optimizaiton and healing simpler and faster. My tutorials to learn DRL includes [Richard Sutton's book](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf), [David Silver's cources](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), [Andrej Karpathy's blog](http://karpathy.github.io/2016/05/31/rl/) and [cs231n](http://cs231n.stanford.edu/).

As for the RL algorithms, I focus on policy gradient (PG) rather than value-based algorithm, e.g. Q-learning. It is because that PG can get [better performance](https://arxiv.org/pdf/1602.01783.pdf) than Q-learning when deep neural network is used as function approximators. Furthermore, [PG can handle the continuous action space](https://arxiv.org/abs/1509.02971). 

In this blog, I will introduce the reasons to propose deterministic policy gradient, how it works and is combinated with deep learning. 

# Policy Gradient
## Stochastic Policy Gradient  

