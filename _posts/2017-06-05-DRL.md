---
layout: post
title: Deep Reinforcement Learning
date: 2017-6-05
excerpt: "Introduction to DQN and DDPG"
comments: true
tags: [reinforcement learning,deep learning, DQN, DDPG] 
---

# Motivation

In the [last post](https://wang90063.github.io/DPG/), I have introduced the policy gradient, including stochastic policy gradient and deterministic policy gradient. In this post, firstly, I will introduce the action-value methods, where the policy in generated from the action-value function. Then, I will introduce DQN and DDPG, which are based on action-value and policy-gradient methods, respectively. 

# Action-Value Methods

In this section, I will introduce some basics for action-value method, including the Bellman (optimal) equations, dynamic programming (DP), Monte-Carlo (MC) Methods and Time-Difference (TD) learning.

## Bellman (Optimality) Equation

Generally, RL is to derive the optimal policy to maximize the accumulative reward.  From the view of optimization, RL is expressed as

$$
\begin{equation}
\mathop {\pi ^*} = \arg\max\limits_\pi  {E_\pi }\left[ {r_0^\gamma \mid {s_0}}  \right]
\end{equation}
$$

where $$r_0^\gamma  = \sum\limits_{t = 0}^T {\gamma ^t{r_t}}$$ is the fixed-horizon reward in a episode. Then, two value functions are give as 

$$ \begin{equation}
{Q^\pi }\left( {s,a} \right) = {E_\pi }\left[ {r_t^\gamma \mid {s_t} = s,{a_t} = a} \right]
\end{equation}$$ 

and 

$$ \begin{equation}
{V^{\pi} }\left( {s,a} \right) = {E_\pi }\left[ {r_t^\gamma \mid {s_t} = s} \right]
\end{equation}$$

where $$r_t^\gamma  = \sum\limits_{k = t}^{T - 1} {\gamma ^{k - t}{r_k }}$$ is the accumulative reward from time step $$t$$. Also $${V^\pi }\left( s \right) = \sum\limits_a {\pi (s,a){Q^\pi }\left( {s,a} \right)} $$. Moreover, the policy $$\pi (s,a)$$ is parameterized by $$\theta$$, i.e. $$\pi_\theta (s,a)$$. Also, the expected rewards for state-action pairs are defined as

$$
\begin{equation}
R(s,a) = \sum\limits_r {r\sum\limits_{s'} {P(s',r \mid s,a)} } 
\end{equation}
$$  

where $$P(s',r \mid s,a)$$ is the probability of each possible pair of next state $$s'$$ and $$r$$ conditioned on current state $$s$$ and action $$a$$.

Moreover, the expected rewards for state-action-next-state pairs is defined as


Hence, the state-transition probability is given as

$$
\begin{equation}
P(s' \mid s,a) = \sum\limits_r {P(s',r \mid s,a)} 
\end{equation}$$

### Bellman Equation

Then, the Bellman equations are given as

$$\begin{array}{l}
{V^\pi }\left( s \right) = {E_\pi }\left[ {\sum\limits_{k = t}^\infty  {\gamma ^{t - k}{r_k}} \mid {s_t} = s} \right] = {E_\pi }\left[ {r_t + \sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k} r_k} \mid {s_t} = s} \right]\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = {E_\pi }({r_t} \mid {s_t} = s) + \gamma {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}{r_k}} \mid {s_t} = s} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_{s'} {\sum\limits_a {\pi (s,a)\sum\limits_r {r \times P(s',r \mid s,a)} } } \\ {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\rm{ + }}\gamma \sum\limits_{s'} {P(s'\mid s)} {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}r_k} \mid s_{t + 1} = s'} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_a {\pi (s,a)\sum\limits_{s',r} {P(s',r \mid s,a)(r + \gamma {V^\pi }\left( {s'} \right)} } )
\end{array}$$

and

$$
\begin{array}{l}
{Q^\pi }\left( {s,a} \right) = {E_\pi }\left[ {\sum\limits_{k = t}^\infty  {\gamma ^{t - k}{r_k}} \mid {s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {r_t + \sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k}{r_k}} \mid {s_t} = s,{a_t} = a} \right]\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = {E_\pi }({r_t} \mid {s_t} = s,{a_t} = a) + \gamma {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}}{r_k} \mid {s_t} = s,{a_t} = a} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_{s',r} {P(s',r \mid s,a)r} \\{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}
{\rm{ + }}\gamma \sum\limits_{a'} {\pi (s',a')\sum\limits_{s',r} {P(s',r \mid s,a)} {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}{r_k}} \mid {s_{t + 1}} = s',a_{t + 1} = a'} \right)} \\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_{s',r} {P(s',r \mid s,a)(r + } \gamma \sum\limits_{a'} {\pi (s',a'){Q^\pi }\left( {s',a'} \right))} 
\end{array}
$$

### Bellman Optimality Equation

$$\begin{array}{l}
{V^*}\left( s \right) = \mathop {\max }\limits_a {Q^*}\left( {s,a} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \mathop {\max }\limits_a \sum\limits_{s',r} {P(s',r \mid s,a)(r + } \gamma {V^*}\left( {s'} \right))
\end{array}$$

## General Policy Iteration

Before introducing DP, MC and TD, I want to give a high-level description of all the action-value Methods, which can clarify the similarites and differences of the three methods.

General Policy Iteration (GPI) constists of two parallel and iteracting processes, which are policy evaluation (PE) and policy improvement (PI). The PE evaluates the value fuctions of current policy for all the states (state-action pairs) and the PI changes the current policy greedily with respect to the evaluated value fuctions. If the two processes stablize, which means that the policy is the greedy policy with respect to its own value function, it is obvious that the convergent policy after GPI satisfies the bellman optimality equation. Hence, the convergent policy is the optimal policy. It is worth noting that the greedy policy is the global optimal policy due the the prediction of value function.
## Dynamic Programming

Now, with the Bellman (Optimality) Equations, let us think the most intuitive way to get optimal policy from state-value function. 