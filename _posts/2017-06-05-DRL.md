---
layout: post
title: Deep Reinforcement Learning
date: 2017-6-05
excerpt: "Introduction to DQN and DDPG"
comments: true
tags: [reinforcement learning,deep learning, DQN, DDPG] 
---

# Motivation

In the [last post](https://wang90063.github.io/DPG/), I have introduced the policy gradient, including stochastic policy gradient and deterministic policy gradient. In this post, firstly, I will introduce the action-value methods, where the policy in generated from the action-value function. Then, I will introduce DQN and DDPG, which are based on action-value and policy-gradient methods, respectively. 

# Action-Value Methods

In this section, I will introduce some basics for action-value method, including the Bellman (optimal) equations, dynamic programming (DP), Monte-Carlo (MC) Methods and Time-Difference (TD) learning.

## Bellman (Optimality) Equation

Generally, RL is to derive the optimal policy to maximize the accumulative reward.  From the view of optimization, RL is expressed as

$$
\begin{equation}
\mathop {\pi ^*} = \arg\max\limits_\pi  {E_\pi }\left[ {r_0^\gamma \mid {s_0}}  \right]
\end{equation}
$$

where $$r_0^\gamma  = \sum\limits_{t = 0}^T {\gamma ^t{r_t}}$$ is the fixed-horizon reward in a episode. Then, two value functions are give as 

$$ \begin{equation}
{Q^\pi }\left( {s,a} \right) = {E_\pi }\left[ {r_t^\gamma \mid {s_t} = s,{a_t} = a} \right]
\end{equation}$$ 

and 

$$ \begin{equation}
{V^{\pi} }\left( {s,a} \right) = {E_\pi }\left[ {r_t^\gamma \mid {s_t} = s} \right]
\end{equation}$$

where $$r_t^\gamma  = \sum\limits_{k = t}^{T - 1} {\gamma ^{k - t}{r_k }}$$ is the accumulative reward from time step $$t$$. Also $${V^\pi }\left( s \right) = \sum\limits_a {\pi (s,a){Q^\pi }\left( {s,a} \right)} $$. Moreover, the policy $$\pi (s,a)$$ is parameterized by $$\theta$$, i.e. $$\pi_\theta (s,a)$$. Also, the expected rewards for state-action pairs are defined as

$$
\begin{equation}
R(s,a) = \sum\limits_r {r\sum\limits_{s'} {P(s',r \mid s,a)} } 
\end{equation}
$$  

where $$P(s',r \mid s,a)$$ is the probability of each possible pair of next state $$s'$$ and $$r$$ conditioned on current state $$s$$ and action $$a$$.

Moreover, the expected rewards for state-action-next-state pairs is defined as


Hence, the state-transition probability is given as

$$
\begin{equation}
P(s' \mid s,a) = \sum\limits_r {P(s',r \mid s,a)} 
\end{equation}$$

### Bellman Equation

Then, the Bellman equations are given as

$$\begin{array}{l}
{V^\pi }\left( s \right) = {E_\pi }\left[ {\sum\limits_{k = t}^\infty  {\gamma ^{t - k}{r_k}} \mid {s_t} = s} \right] = {E_\pi }\left[ {r_t + \sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k} r_k} \mid {s_t} = s} \right]\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = {E_\pi }({r_t} \mid {s_t} = s) + \gamma {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}{r_k}} \mid {s_t} = s} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_{s'} {\sum\limits_a {\pi (s,a)\sum\limits_r {r \times P(s',r \mid s,a)} } } \\ {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\rm{ + }}\gamma \sum\limits_{s'} {P(s'\mid s)} {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}r_k} \mid s_{t + 1} = s'} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_a {\pi (s,a)\sum\limits_{s',r} {P(s',r \mid s,a)(r + \gamma {V^\pi }\left( {s'} \right)} } )
\end{array}$$

and

$$
\begin{array}{l}
{Q^\pi }\left( {s,a} \right) = {E_\pi }\left[ {\sum\limits_{k = t}^\infty  {\gamma ^{t - k}{r_k}} \mid {s_t} = s,{a_t} = a} \right] = {E_\pi }\left[ {r_t + \sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k}{r_k}} \mid {s_t} = s,{a_t} = a} \right]\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = {E_\pi }({r_t} \mid {s_t} = s,{a_t} = a) + \gamma {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}}{r_k} \mid {s_t} = s,{a_t} = a} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_{s',r} {P(s',r \mid s,a)r} \\{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}
{\rm{ + }}\gamma \sum\limits_{a'} {\pi (s',a')\sum\limits_{s',r} {P(s',r \mid s,a)} {E_\pi }\left( {\sum\limits_{k = t + 1}^\infty  {\gamma ^{t - k - 1}{r_k}} \mid {s_{t + 1}} = s',a_{t + 1} = a'} \right)} \\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \sum\limits_{s',r} {P(s',r \mid s,a)(r + } \gamma \sum\limits_{a'} {\pi (s',a'){Q^\pi }\left( {s',a'} \right))} 
\end{array}
$$

### Bellman Optimality Equation

$$\begin{array}{l}
{V^*}\left( s \right) = \mathop {\max }\limits_a {Q^*}\left( {s,a} \right)\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}  = \mathop {\max }\limits_a \sum\limits_{s',r} {P(s',r \mid s,a)(r + } \gamma {V^*}\left( {s'} \right))
\end{array}$$

## General Policy Iteration

Before introducing DP, MC and TD, I want to give a high-level description of all the action-value Methods, which can clarify the similarites and differences of the three methods.

General Policy Iteration (GPI) constists of two parallel and iteracting processes, which are policy evaluation (PE) and policy improvement (PI). The PE evaluates the value fuctions of current policy for all the states (state-action pairs) and the PI changes the current policy greedily with respect to the evaluated value fuctions. If the two processes stablize, which means that the policy is the greedy policy with respect to its own value function, it is obvious that the convergent policy after GPI satisfies the bellman optimality equation. Hence, the convergent policy is the optimal policy. It is worth noting that the greedy policy is the global optimal policy due to the prediction of value function. 

![GPI](https://jaydottechdotblog.files.wordpress.com/2016/12/rl-general-policy-iteration-diagram.png)

## Dynamic Programming

### Policy Evaluation/Prediction
Okay, following the idea of GPI, the DP is easy to understand. If the environment dynamic $$P(s',r \mid s,a)$$ is known, the value functions for all the states with a given policy $$\pi$$ can be obtained by solving $$\mid S \mid$$ linear equations in $$\mid S \mid$$ unknowns $${V^\pi }\left( s \right)$$, according to the Bellman equation for $${V^\pi }\left( s \right)$$, where $$\mid S \mid$$ is the size of state space. However, in order to make it more friendly to programming, an iterative PE method is proposed. Specifically, first of all, an arbitrary initial $${V^{0} }\left( s \right)$$ is given. Then, Bellman equation is considered as the update principle for any state,  

$$\begin{equation}
{V^{k+1} }\left( s \right) = \sum\limits_a {\pi (s,a)\sum\limits_{s',r} {P(s',r \mid s,a)(r + \gamma {V^{k} }\left( {s'} \right)} } )
\end{equation}$$.

### Policy Improvement
From GPI, the policy improvement is greedy. Of course, as mentioned above, the greedy method satisfies the Bellman optimality equation. But  Can it ensure the convergency?? The answer is yes!! In the following, the policy improvement therem is given to prove the convergency. First of all, the fact that the greedy policy can only derive deterministic policy, which means that for a given state the policy only choose one single action. 

Therefore, let us assume that we have a state $$s$$, and the current policy is $$$\pi(s)$$. We want to change the action $$a$$ from $$s$$, i.e. $$a \ne \pi(s)$$, using greedy policy  $$\pi'(s)$$ with respect to the state-value function $${V^{\pi} }\left( s \right)$$ under current policy $$\pi(s)$$. Also, the following states follows the current policy $$\pi$$. Hence, 

$$
\begin{equation}
{Q^\pi }\left( {s,\pi'(s)} \right) = \max\limits_a \sum\limits_{s',r} {P(s',r \mid s,a)(r + } \gamma V(s') )
\end{equation}
$$

The $${V^\pi }(s)$$ is the average over $$\pi(s)$$, while $${Q^\pi }\left( {s,\pi'(s)} \right)$$ choose the action to maximize the prediction. It is obvious that 

$$
\begin{equation}
{V^\pi }(s) \le {Q^\pi }\left( {s,\pi '(s)} \right)
\end{equation}
$$. 

If the greedy policy is applied in every state, we can get

$$\begin{array}{l}
{V^\pi }({s_t}) \le  {Q^\pi }\left( {s_t,\pi '({s_t})} \right) = \mathop {\max }\limits_{a_t} \sum\limits_{s_{t + 1},{r_t}} {P({s_{t + 1}},{r_t} \mid {s_t},{a_t})({r_t} + } \gamma {V^\pi }\left( s_{t + 1} \right))\\
{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}   \le \mathop {\max }\limits_{a_t} \sum\limits_{s_{t + 1},r_t} P({s_{t + 1}},{r_t} \mid {s_t},{a_t})\\{\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt}({r_t} +  \gamma \mathop {\max }\limits_{a_{t + 1}} \sum\limits_{s_{t + 2},r_{t + 1}} {P({s_{t + 2}},{r_{t + 1}} \mid {s_{t + 1}},{a_{t + 1}})({r_{t + 1}} + } \gamma {V^\pi }\left( {s_{t + 2}} \right))) \\
\;\;\;\;\;\;\;\;\;\;......\\
\;\;\;\;\;\;\;\;\;\; \le\sum\limits_{s_{t + k},r_{t + k}} {\sum\limits_{k = 1}^T {\gamma ^k p({s_t} \to {s_{t + k}},{r_{t + k}},\pi')} } {r_{t + k}} = {V^{\pi '}}({s_t})
\end{array}$$

Therefore, if $$\pi$$ and $$\pi'$$ are two deterministic policies, for all $$s \in S$$, and satisfy $${V^\pi }(s) \le {Q^\pi }\left( {s,\pi '(s)} \right)$$, then $$V^\pi(s) \le V^{\pi'}(s)$$.

### Policy Iteration/Control

The policy iteration for DP is easy to obtain from GPI, iterative PE and greedy PI. Specifically, from the initial policy and state-value function, the iterative PE is used to evaluate the state-value function under current policy. Then, the greedy policy is used to improve the policy with the state-value function from iterative PE.

### Existence and Uniqueness

In the above, I only prove the policy improvement theorem. However, in policy iteration, the two processes, i.e. PE and PI, should converge. Specifically, for PE, how many solution for the linear equation system? Can iterative PE converge to these solutions? According to the [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf) from David Silver,  the two questions are answered in the following.

First of all, the value function space $$\Omega $$ is defined, where each point is a $$\mid S \mid$$-dimension vector and fully specified by value function $$V(s)$$, for all $$s \in S $$. Also, in this space, the distance between two points $$U$$ and $$V$$ is $$\infty-norm$$, i.e.

$$
\begin{equation}
d(U,V)=\left\| {U - V} \right\|_\infty 
\end{equation}
$$

For iterative PE, the Bellman expectation backup operator $$T^\pi$$ is defined as 

$$\begin{align}
T^\pi(V) = \sum\limits_a {\pi (s,a)\sum\limits_{s',r} {P(s',r \mid s,a)(r + \gamma {V^{k} }\left( {s'} \right)} } )\\
=R^\pi(s,a) + \gamma P^\pi(s',r \mid s,a)V 
\end{align}$$ 

Also, a point $$v \in \Omega$$ such that

$$
\begin{equation}
T^\pi(V) = V
\end{equation}
$$

is called a fixed point.

Now, for any two poins $$U$$ and $$V$$ in $$\Omega$$, the operator $$T^\pi$$ is $$\gamma-contraction$$ due to 

$$
\begin{equation}
{\left\| {T^\pi (U) - {T^\pi }(V)} \right\|_\infty } \le \gamma {\left\| {U - V} \right\|_\infty }
\end{equation}
$$

where $$ 0 < \gamma < 1$$. 

Therefore, from the contraction mapping theorem, the fixed point for $$\gamma-contraction$$ operator $$T^\pi$$ is unique. Also, $$T^\pi$$ converges at a linear rate of $$\gamma$$.

Therefore, PI theorem ensures that the policy updates to the optimal policy and contraction mapping theorem ensures iterative PE to be consistent with the current given policy. The two theorems can prove that the policy iteration converges to the optimal policy to maximize the state-value function for all the states. 

### Value Iteration/Control

It is not efficient to use the iterative PE until converging after every PI. One way to solve this problem is to truncate iterative PE. The most exetrem example is to do the iteration once after each PE, i.e.

$$\begin{equation}
{V^{k+1} }\left( s \right) = \max\limits_a {\sum\limits_{s',r} {P(s',r \mid s,a)(r + \gamma {V^{k} }\left( {s'} \right)} } )
\end{equation}$$

This equation can do the PE and PI simultaneously. Also, the Bellman optimality backup operator $$T^*(V) = \max\limits_a {\sum\limits_{s',r} {P(s',r \mid s,a)(r + \gamma V} })$$ is $$\gamma-contraction$$. Hence, from contraction mapping theorem, this operator converges to the unique fixed point $$T^*(V) = V$$, which is the state-value function for the optimal policy.

## Monte-Carlo Methods

All the above algorithms in DP are based on a fact: the environment's dynamic is prior known, i.e.  $$P(s',r \mid s,a)$$ is known all the time. In surprisingly many cases it is easy to generate experience sampled according to the desired probability distributions, but infeasible to obtain the distributions in explicit form. Hence, the model-free methods to sample from the environment is applicable in many cases. 

Before proceeding, it is worth noting that the action-value function $$ Q(s,a)$$ is used rather than state-value function $$V(s)$$. If using $$V(s)$$, the greedy policy improvement needs $$P(s',r \mid s,a)$$, which is impossible in model-free method. Hence, only $$ Q(s,a)$$ can be used.

### Policy Evaluation/Prediction

For a given deterministic policy $$\pi$$, MCPE evaluates $$ Q^\pi(s,a)$$ for all the state-action pairs. The intuitive MCPE is to generate infinite episodes covering all the state-action pairs and average the sampled returns with respect to the first visiting times (firs-visit MC). The sampled returns for $$(s,a)$$ is an unbiased, independent and identically estimae of $$ Q^\pi(s,a)$$ with finite variance. 

However, the intuitive way has two problems: the first problem is that the assumption for infinite episodes cannot be satisfied. The seconde problem is that a deterministic fixed policy cannot walk through all the state-action pairs.  